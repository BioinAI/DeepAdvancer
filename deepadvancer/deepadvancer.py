import os
import pandas as pd
import numpy as np
from collections import Counter
import glob
from sklearn.preprocessing import LabelEncoder
from rpy2.robjects import pandas2ri, default_converter
from rpy2.robjects.conversion import localconverter
import rpy2.robjects as robjects
from collections import defaultdict
from scipy.stats import pearsonr
import torch.nn as nn
import torch
import random
from tqdm import tqdm
from torch.optim import Adam
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
from matplotlib import cm
from sklearn.decomposition import PCA
from importlib import resources

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def load_and_process_expression_data(
    data_dir,
    output_dir=None,
    gene_threshold_ratio=0.8,
    dataset_gene_coverage_threshold=0.8
):
    """
    è¯»å–å¹¶æ•´åˆä¸€ä¸ªç›®å½•ä¸‹çš„å¤šä¸ªè¡¨è¾¾çŸ©é˜µå’Œè¡¨å‹æ–‡ä»¶ï¼Œå¹¶å®Œæˆæ¸…æ´—å’Œå½’ä¸€åŒ–ã€‚

    å‚æ•°ï¼š
        data_dir: str
            ç›®å½•è·¯å¾„ï¼Œæ¯ä¸ªå­æ–‡ä»¶å¤¹åŒ…å«ä¸€ä¸ªè¡¨è¾¾çŸ©é˜µï¼ˆåŒ…å«"expression"ï¼‰å’Œä¸€ä¸ªè¡¨å‹æ–‡ä»¶ï¼ˆåŒ…å«"phenotype"ï¼‰ï¼Œå‡ä¸º.csvæ ¼å¼ã€‚
        output_dir: str or None
            å¦‚æœè®¾ç½®ï¼Œå°†ç»“æœä¿å­˜åˆ°è¯¥ç›®å½•ä¸‹ã€‚
        gene_threshold_ratio: float
            å®šä¹‰â€œå…±æœ‰åŸºå› â€çš„é˜ˆå€¼ï¼ˆé»˜è®¤80%æ•°æ®é›†ä¸­éƒ½å­˜åœ¨çš„åŸºå› ï¼‰ã€‚
        dataset_gene_coverage_threshold: float
            æ¯ä¸ªæ•°æ®é›†åŒ…å«å¤šå°‘æ¯”ä¾‹å…±æœ‰åŸºå› æ‰ä¿ç•™ï¼ˆé»˜è®¤80%ï¼‰ã€‚

    è¿”å›ï¼š
        final_expression_matrix_scaled: pd.DataFrame
            å½’ä¸€åŒ–åçš„è¡¨è¾¾çŸ©é˜µ
        phenotype_metadat_scaled: pd.DataFrame
            å¯¹åº”çš„æ ·æœ¬è¡¨å‹æ•°æ®
    """

    expression_data = {}
    phenotype_data = {}

    # è¯»å–æ‰€æœ‰è¡¨è¾¾å’Œè¡¨å‹æ–‡ä»¶
    for folder_name in os.listdir(data_dir):
        folder_path = os.path.join(data_dir, folder_name)
        if os.path.isdir(folder_path):
            expr_files = glob(os.path.join(folder_path, '*expression*.csv'))
            pheno_files = glob(os.path.join(folder_path, '*phenotype*.csv'))

            if expr_files and pheno_files:
                expr_file = expr_files[0]
                pheno_file = pheno_files[0]

                expr_df = pd.read_csv(expr_file, index_col=0)
                pheno_df = pd.read_csv(pheno_file, index_col=0)

                expression_data[folder_name] = expr_df
                phenotype_data[folder_name] = pheno_df

    print(f"å…±è¯»å–åˆ° {len(expression_data)} ä¸ªæ•°æ®é›†ã€‚")

    # æ‰¾å‡ºå…±æœ‰åŸºå› 
    gene_sets = [set(df.index) for df in expression_data.values()]
    total_datasets = len(gene_sets)
    gene_counter = Counter(g for genes in gene_sets for g in genes)
    gene_threshold = int(total_datasets * gene_threshold_ratio)
    common_genes = [gene for gene, count in gene_counter.items() if count >= gene_threshold]
    print(f"å…±æœ‰åŸºå› æ•°ï¼ˆå‡ºç°åœ¨ â‰¥{gene_threshold_ratio:.0%} æ•°æ®é›†ä¸­ï¼‰: {len(common_genes)}")

    # ç­›é€‰ä¿ç•™çš„æ•°æ®é›†
    filtered_expression_data = {}
    filtered_phenotype_data = {}

    for dataset_name, expr_df in expression_data.items():
        gene_overlap = set(common_genes).intersection(expr_df.index)
        overlap_ratio = len(gene_overlap) / len(common_genes)

        if overlap_ratio >= dataset_gene_coverage_threshold:
            filtered_expr_df = expr_df.loc[list(gene_overlap)].copy()
            filtered_expression_data[dataset_name] = filtered_expr_df
            filtered_phenotype_data[dataset_name] = phenotype_data[dataset_name]
            print(f"âœ… ä¿ç•™æ•°æ®é›†: {dataset_name}ï¼ˆå…±æœ‰åŸºå› æ¯”ä¾‹ {overlap_ratio:.2%}ï¼‰")
        else:
            print(f"âŒ ä¸¢å¼ƒæ•°æ®é›†: {dataset_name}ï¼ˆå…±æœ‰åŸºå› æ¯”ä¾‹ {overlap_ratio:.2%}ï¼‰")

    # åˆå¹¶è¡¨è¾¾çŸ©é˜µ
    #expression_combined = pd.concat(
    #    [df.reindex(common_genes).fillna(0) for df in filtered_expression_data.values()],
     #   axis=1
    #)
    
    # å…ˆå°†æ‰€æœ‰è¡¨è¾¾çŸ©é˜µå¯¹é½è‡³ common_genesï¼ˆä¿ç•™ NaNï¼‰
    aligned_dfs = [df.reindex(common_genes) for df in filtered_expression_data.values()]

    # æ‹¼æ¥ä¸ºä¸´æ—¶å¤§çŸ©é˜µï¼ˆåˆ—æ˜¯æ ·æœ¬ï¼Œè¡Œæ˜¯åŸºå› ï¼‰
    expression_combined = pd.concat(aligned_dfs, axis=1)

    # æŒ‰è¡Œï¼ˆæ¯ä¸ªåŸºå› ï¼‰å¡«è¡¥ NaN ä¸ºè¯¥åŸºå› çš„å¹³å‡è¡¨è¾¾ï¼ˆè·¨æ ·æœ¬ï¼‰
    expression_combined = expression_combined.apply(lambda row: row.fillna(row.mean()), axis=1)
   
    

    # åˆå¹¶è¡¨å‹ä¿¡æ¯
    all_pheno = []
    for dataset_name, df in filtered_phenotype_data.items():
        df = df.copy()
        df['source_dataset'] = dataset_name
        all_pheno.append(df)

    phenotype_combined = pd.concat(all_pheno, axis=0)

    # å»é‡
    expression_combined = expression_combined.loc[:, ~expression_combined.columns.duplicated()]
    phenotype_combined = phenotype_combined.loc[~phenotype_combined.index.duplicated(keep='first')]

    # å¯¹é½æ ·æœ¬
    shared_samples = expression_combined.columns.intersection(phenotype_combined.index)
    expression_combined = expression_combined[shared_samples]
    phenotype_combined = phenotype_combined.loc[shared_samples]

    # æ¸…æ´—å˜é‡å
    sample_metadat_cleaned = phenotype_combined
    expression_matrix_cleaned = expression_combined

    # é¢„å¤„ç†è¡¨è¾¾æ•°æ®
    grouped = sample_metadat_cleaned.groupby('source_dataset')
    processed_datasets = []

    for i, (source, group) in enumerate(grouped, 1):
        print(f"Processing dataset {i}: {source}")
        sample_ids = group.index
        sub_matrix = expression_matrix_cleaned[sample_ids].copy()

        #same_value_genes = sub_matrix.nunique(axis=1) == 1
        #sub_matrix.loc[same_value_genes] = 0

        def handle_outliers(matrix):
            mean_value = np.nanmean(matrix.values)
            max_value = np.nanmax(matrix.values)
            if max_value > 10 * mean_value:
                lower_bound = np.nanquantile(matrix.values, 0.01)
                upper_bound = np.nanquantile(matrix.values, 0.99)
                print(f"Clipping values outside [{lower_bound}, {upper_bound}].")
                return matrix.clip(lower=lower_bound, upper=upper_bound)
            else:
                return matrix

        sub_matrix = handle_outliers(sub_matrix)

        def needs_log_transform(matrix):
            qx = np.nanquantile(matrix.values.flatten(), [0.0, 0.25, 0.5, 0.75, 0.99, 1.0])
            return (qx[5] > 100) or ((qx[5] - qx[0] > 50) and (qx[1] > 0))

        if needs_log_transform(sub_matrix):
            print(f"Dataset {i}: {source} requires log transformation.")
            #sub_matrix[sub_matrix <= 0] = np.nan
            #sub_matrix = np.log2(sub_matrix)
            #sub_matrix = sub_matrix.fillna(0)
            min_val = sub_matrix.min().min()
            if min_val <= 0:
                shift = abs(min_val) + 1e-3
                sub_matrix += shift
            sub_matrix = np.log2(sub_matrix)


        processed_datasets.append(sub_matrix)

    # åˆå¹¶å¤„ç†åçš„è¡¨è¾¾æ•°æ®
    final_expression_matrix = pd.concat(processed_datasets, axis=1)
    print("All datasets processed and combined.")

    # åˆ é™¤è¡¨è¾¾å€¼ç›¸åŒçš„åŸºå› ï¼ˆ80% æ ·æœ¬ç›¸åŒå€¼ï¼‰
    rows_to_remove = []
    for row in final_expression_matrix.index:
        value_counts = final_expression_matrix.loc[row].value_counts(normalize=True)
        if value_counts.max() >= 0.6:
            rows_to_remove.append(row)

    final_expression_matrix = final_expression_matrix.drop(index=rows_to_remove)
    #sample_metadat_cleaned = sample_metadat_cleaned.drop(index=rows_to_remove)
    print(f"Removed {len(rows_to_remove)} rows with low expression variance.")

    # æŒ‰æ•°æ®é›†è¿›è¡Œ min-max å½’ä¸€åŒ–
    normalized_datasets = []

    for dataset_name, group in sample_metadat_cleaned.groupby('source_dataset'):
        sample_ids = group.index.tolist()
        valid_samples = [s for s in sample_ids if s in final_expression_matrix.columns]
        if not valid_samples:
            continue
        sub_expr = final_expression_matrix[valid_samples].copy()
        dataset_max = sub_expr.values.max()
        dataset_min = sub_expr.values.min()
        if dataset_max == dataset_min:
            print(f"âš ï¸ ä¸¢å¼ƒæ— ä¿¡æ¯æ•°æ®é›†: {dataset_name}")
            continue
        sub_expr_scaled = sub_expr#(sub_expr - dataset_min) / (dataset_max - dataset_min)
        normalized_datasets.append(sub_expr_scaled)

    final_expression_matrix_scaled = pd.concat(normalized_datasets, axis=1)

    # ä¿ç•™æ ·æœ¬å¹¶å¯¹é½
    retained_samples = final_expression_matrix_scaled.columns
    phenotype_metadat_scaled = sample_metadat_cleaned.loc[
        sample_metadat_cleaned.index.intersection(retained_samples)
    ]
    phenotype_metadat_scaled = phenotype_metadat_scaled.loc[retained_samples]

    print(f"å½’ä¸€åŒ–åè¡¨è¾¾çŸ©é˜µç»´åº¦: {final_expression_matrix_scaled.shape}")
    print(f"å½’ä¸€åŒ–åè¡¨å‹æ•°æ®ç»´åº¦: {phenotype_metadat_scaled.shape}")
    
     # Step 9: ä¿å­˜åˆ°è¾“å‡ºè·¯å¾„
    final_expression_matrix_scaled.to_csv(os.path.join(output_dir, "expression_matrix_scaled.csv"))
    phenotype_metadat_scaled.to_csv(os.path.join(output_dir, "phenotype_metadat_scaled.csv"))
    print(f"âœ… ä¿å­˜æˆåŠŸ: æ–‡ä»¶å·²å†™å…¥ {output_dir}")

    return final_expression_matrix_scaled, phenotype_metadat_scaled

def showloss(loss_list, title="Loss Curve"):
    plt.figure(figsize=(6, 4))
    plt.plot(loss_list)
    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.show()

def calculate_gene_expression(adjusted_z, sigmatrix, class_labels, calculate_intermediate=True):
    """
    æ ¹æ®è°ƒæ•´åçš„ Z å’Œ sigmatrix è®¡ç®—åŸºå› è¡¨è¾¾å€¼ã€‚
    :param adjusted_z: è°ƒæ•´åçš„ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º (batch_size, feature_dim, num_genes)
    :param sigmatrix: çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (feature_dim, num_genes)
    :param class_labels: æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (batch_size,)
    :param calculate_intermediate: æ˜¯å¦è®¡ç®—ä¸­é—´ç»“æœï¼Œé»˜è®¤ä¸º True
    :return: åŸºå› è¡¨è¾¾å€¼ï¼Œå½¢çŠ¶ä¸º (batch_size, num_genes)
             å¦‚æœ calculate_intermediate ä¸º Trueï¼Œè¿˜è¿”å› intermediate_result
    """
    batch_size, feature_dim, num_genes = adjusted_z.size()

    # åˆå§‹åŒ–åŸºå› è¡¨è¾¾å¼ é‡
    gene_expression = torch.zeros((batch_size, num_genes), device=adjusted_z.device)

    # å¦‚æœéœ€è¦è®¡ç®— intermediate_resultï¼Œåˆ™åˆå§‹åŒ–å®ƒ
    if calculate_intermediate:
        intermediate_result = torch.zeros((batch_size, feature_dim, num_genes), device=adjusted_z.device)

    for i in range(batch_size):
        # å½“å‰æ ·æœ¬çš„è°ƒæ•´ç‰¹å¾ (feature_dim, num_genes)
        current_adjusted_z = adjusted_z[i]  # (feature_dim, num_genes)
        sample_class = class_labels[i].item()  # å½“å‰æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾
        #print(sample_class)
        
        #if calculate_intermediate:
        #    feature_weights = torch.ones(feature_dim, device=adjusted_z.device)  # åˆå§‹åŒ–ç‰¹å¾æƒé‡ä¸º1

            # æŒ‰ç‰¹å¾é€è¡Œè®¡ç®—
       #     for j in range(feature_dim):
                # å¦‚æœæ ·æœ¬ç±»åˆ«å±äºå½“å‰ç‰¹å¾çš„ç±»æ ‡ç­¾é›†åˆï¼Œå°†æ¯”ä¾‹è°ƒæ•´ä¸º8å€
       #         if sample_class in feature_class_sets[j]:
      #              feature_weights[j] = 100.0

                # å½“å‰ç‰¹å¾å€¼å’Œ sigmatrix å¯¹åº”è¡Œç›¸ä¹˜
      #          intermediate_result[i, j] = current_adjusted_z[j] * sigmatrix[j]  # (num_genes,)

            # å¯¹ç‰¹å¾æƒé‡è¿›è¡Œå½’ä¸€åŒ–
       #     feature_weights /= feature_weights.sum()
            #print(feature_weights)
            
            # å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œç´¯åŠ ï¼ŒæŒ‰ç…§å½’ä¸€åŒ–çš„æƒé‡è°ƒæ•´
       #     gene_expression[i] = (intermediate_result[i] * feature_weights.unsqueeze(-1)).sum(dim=0)
      #  else:
            # å¦‚æœä¸éœ€è¦ä¸­é—´ç»“æœï¼Œç›´æ¥è®¡ç®—åŸºå› è¡¨è¾¾å€¼
     #       gene_expression[i] = torch.matmul(current_adjusted_z, sigmatrix.T).sum(dim=0)

        if calculate_intermediate:
            # æŒ‰ç‰¹å¾é€è¡Œè®¡ç®—
            for j in range(feature_dim):
                # å½“å‰ç‰¹å¾å€¼å’Œ sigmatrix å¯¹åº”è¡Œç›¸ä¹˜
                intermediate_result[i, j] = current_adjusted_z[j] * sigmatrix[j]  # (num_genes,)

            # å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œç´¯åŠ ï¼Œè°ƒæ•´ç‰¹å¾1çš„æ¯”ä¾‹ä¸º 3 å€
            gene_expression[i] = intermediate_result[i].sum(dim=0) + 3 * intermediate_result[i, 0] # + 0.1 * b4 # ç‰¹å¾1æ¯”ä¾‹å˜ä¸º 3 å€
        else:
            # å¦‚æœä¸éœ€è¦ä¸­é—´ç»“æœï¼Œç›´æ¥è®¡ç®—åŸºå› è¡¨è¾¾å€¼
            gene_expression[i] = torch.matmul(current_adjusted_z, sigmatrix.T).sum(dim=0) #+ b4 
            

    if calculate_intermediate:
        return gene_expression, intermediate_result
    else:
        return gene_expression 
    
def adjust_features(z, class_sub, proportions_per_feature):
    """
    æ ¹æ® class_sub å’Œ proportions_per_feature ä¸ºæ¯ä¸ªç‰¹å¾æ„å»ºè°ƒæ•´çŸ©é˜µå¹¶åº”ç”¨åˆ° zã€‚
    :param z: ç¼–ç åçš„ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º (batch_size, feature_dim)
    :param class_sub: å½“å‰æ‰¹æ¬¡çš„ç±»æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (batch_size,)
    :param proportions_per_feature: æ¯ä¸ªç‰¹å¾çš„è°ƒæ•´æ¯”ä¾‹ï¼Œæ˜¯ä¸€ä¸ª (feature_dim, num_classes, num_genes) çš„å¼ é‡
    :return: è°ƒæ•´åçš„ç‰¹å¾ï¼Œè¿”å›ä¸€ä¸ª (batch_size, feature_dim, num_genes) çš„å¼ é‡
    """
    # ç¡®ä¿ proportions_per_feature ä¸ z åœ¨åŒä¸€è®¾å¤‡ä¸Š
    proportions_per_feature = proportions_per_feature.to(z.device)
    
    # è·å–å¼ é‡å½¢çŠ¶
    batch_size, feature_dim = z.size()  # (batch_size, feature_dim)
    num_classes, num_genes = proportions_per_feature.size(1), proportions_per_feature.size(2)

    # åˆ›å»º one-hot ç¼–ç çŸ©é˜µï¼Œç”¨äºé€‰æ‹©å½“å‰ç±»æ ‡ç­¾çš„è°ƒæ•´æ¯”ä¾‹
    class_one_hot = F.one_hot(class_sub, num_classes=num_classes).float()  # (batch_size, num_classes)
    
    # é€‰æ‹©æ¯ä¸ªæ ·æœ¬å¯¹åº”ç±»æ ‡ç­¾çš„è°ƒæ•´æ¯”ä¾‹
    # ä½¿ç”¨å¼ é‡ä¹˜æ³•ï¼šclass_one_hot -> (batch_size, num_classes)
    # proportions_per_feature -> (feature_dim, num_classes, num_genes)
    # è¾“å‡ºçš„ selected_proportions -> (batch_size, feature_dim, num_genes)
    selected_proportions = torch.einsum('bc,fcg->bfg', class_one_hot, proportions_per_feature)

    # å°†ç¼–ç ç‰¹å¾ z æ‰©å±•ä¸ºä¸ selected_proportions ç›¸åŒçš„å½¢çŠ¶
    z_expanded = z.unsqueeze(-1)  # (batch_size, feature_dim, 1)
    
    # å¹¿æ’­è®¡ç®—è°ƒæ•´åçš„ç‰¹å¾å€¼
    adjusted_features = z_expanded * selected_proportions  # (batch_size, feature_dim, num_genes)
        # æ·»åŠ éçº¿æ€§æ¿€æ´»å‡½æ•°
    #adjusted_features = F.relu(adjusted_features)  # ReLU æ¿€æ´»å‡½æ•°

    return adjusted_features 

class Discriminator(nn.Module):
    def __init__(self, input_dim, num_outputs):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.CELU(),
            nn.Linear(64, 32),
            nn.CELU(),
            nn.Linear(32, num_outputs),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.layers(x)

class AutoEncoder(nn.Module):
    def __init__(self, input_dim, output_dim, num_batches,num_classes):
        super().__init__()
        self.name = 'ae'
        self.state = 'train' # or 'test'
        self.inputdim = input_dim
        self.outputdim = output_dim
        self.num_batches = num_batches
        self.num_classes = num_classes
        self.encoder = nn.Sequential(nn.Dropout(),
                                     nn.Linear(self.inputdim, 512),
                                     nn.CELU(),
                                     
                                     
                                     nn.Dropout(),
                                     nn.Linear(512, 256),
                                     nn.CELU(),
                                     
                                     
                                     nn.Dropout(),
                                     nn.Linear(256, 128),
                                     nn.CELU(),
                                     
                                     
                                     nn.Dropout(),
                                     nn.Linear(128, 64),
                                     nn.CELU(),
                                     
                                     nn.Linear(64, output_dim),
                                     )
        

        self.decoder = nn.Sequential(nn.Linear(self.outputdim, 64, bias=False),
                                     #nn.CELU(),
                                     nn.Linear(64, 128, bias=False),
                                     #nn.CELU(),
                                     nn.Linear(128, 256, bias=False),
                                     #nn.CELU(),
                                     nn.Linear(256, 512, bias=False),
                                     #nn.CELU(),
                                     nn.Linear(512, self.inputdim, bias=False)
                                     #nn.Sigmoid()  # Added Sigmoid activation here
                                    )

        
        # Batch discriminator
        self.batch_discriminator = Discriminator(output_dim, num_batches)

        # Class discriminator
        self.class_discriminator = Discriminator(output_dim, num_classes)  
        
    def encode(self, x):
        return self.encoder(x)

    def decode(self, z):
        return self.decoder(z)

    def refraction(self,x):
        x_sum = torch.sum(x, dim=1, keepdim=True)
        return x/x_sum

    #def sigmatrix(self):
    #    w0 = (self.decoder[0].weight.T)
    #    w1 = (self.decoder[1].weight.T)
    #    w2 = (self.decoder[2].weight.T)
    #    w3 = (self.decoder[3].weight.T)
    #    w4 = (self.decoder[4].weight.T)
    #    w01 = (torch.mm(w0, w1))
     #   w02 = (torch.mm(w01, w2))
    #    w03 = (torch.mm(w02, w3))
    #    w04 = (torch.mm(w03, w4))
     #   return F.relu(w04)
    
    def sigmatrix_with_bias(self):
        # è·å–æ¯ä¸€å±‚çš„æƒé‡å’Œåç½®
        w0 = self.decoder[0].weight.T 
        #b0 = self.decoder[0].bias    
        w1 = self.decoder[1].weight.T  
        #b1 = self.decoder[1].bias     
        w2 = self.decoder[2].weight.T 
        #b2 = self.decoder[2].bias     
        w3 = self.decoder[3].weight.T  
        #b3 = self.decoder[3].bias     
        w4 = self.decoder[4].weight.T  
        #b4 = self.decoder[4].bias    

        # é€å±‚è®¡ç®—å¹¶åº”ç”¨åç½®
        w01 = torch.mm(w0, w1)  # å½¢çŠ¶: (z_dim, 128)
        #out01 = w01 + b1.unsqueeze(0)  # åç½®åŠ åˆ°ç¬¬ä¸€å±‚è¾“å‡º

        w02 = torch.mm(w01, w2)  # å½¢çŠ¶: (z_dim, 256)
        #out02 = w02 + b2.unsqueeze(0)  # åç½®åŠ åˆ°ç¬¬äºŒå±‚è¾“å‡º

        w03 = torch.mm(w02, w3)  # å½¢çŠ¶: (z_dim, 512)
        #out03 = w03 + b3.unsqueeze(0)  # åç½®åŠ åˆ°ç¬¬ä¸‰å±‚è¾“å‡º

        w04 = torch.mm(w03, w4)  # å½¢çŠ¶: (z_dim, inputdim)
        #out04 = w04 + b4.unsqueeze(0)  # åç½®åŠ åˆ°ç¬¬å››å±‚è¾“å‡º

        return w04  # è¿”å›æœ€ç»ˆçš„ç»“æœ


   # def forward(self, x, class_sub, proportions_per_feature):
    #    sigmatrix = self.sigmatrix()
    #    z = self.encode(x)
        #print(z.size())       
        
        # Predict batch and class
    #    batch_pred = self.batch_discriminator(z)
    #    class_pred = self.class_discriminator(z)

        # è°ƒç”¨å‡½æ•°
    #    adjusted_features = adjust_features(z, class_sub, proportions_per_feature)        
            
    #    x_recon, intermediate_result = calculate_gene_expression(adjusted_features, sigmatrix)

    #    return x_recon, z, batch_pred, class_pred, sigmatrix, intermediate_result
    
    def forward(self, x, expected_sigmatrix, class_sub, proportions_per_feature, calculate_intermediate):
        sigmatrix = self.sigmatrix_with_bias()
        #print(b04.shape)
        z = self.encode(x)
        #x_recon1 = self.decode(z)

        # Predict batch and class
        batch_pred = self.batch_discriminator(z)
        class_pred = self.class_discriminator(z)

        # è°ƒç”¨å‡½æ•°
        calculate_intermediate = self.state == 'train'
        adjusted_features = adjust_features(z, class_sub, proportions_per_feature)

        #x_recon = calculate_gene_expression(adjusted_features, sigmatrix, calculate_intermediate)
        x_recon = calculate_gene_expression(adjusted_features, sigmatrix ,class_sub, calculate_intermediate)

        if calculate_intermediate:
            gene_expression, intermediate_result = x_recon
            return gene_expression, z, batch_pred, class_pred, sigmatrix, intermediate_result
        else:
            gene_expression = x_recon1
            return gene_expression, z, batch_pred, class_pred, sigmatrix    

class SimDataset1(Dataset):
    def __init__(self, X, class_labels_all, batch_labels_all):
        self.X = X  # è¾“å…¥æ•°æ®
        self.class_labels_all = class_labels_all  # ç±»åˆ«æ ‡ç­¾
        self.batch_labels_all = batch_labels_all  # æ‰¹æ¬¡æ ‡ç­¾

    def __len__(self):
        return len(self.X)

    def __getitem__(self, index):
        x = torch.from_numpy(self.X[index]).float()
        class_label_all = torch.tensor(self.class_labels_all[index]).long()
        batch_label_all = torch.tensor(self.batch_labels_all[index]).long()

        # å°†æ•°æ®ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Š
        return x.to(device), class_label_all.to(device), batch_label_all.to(device)
    
def training_stage1(model, train_loader, optimizer_ae, optimizer_disc_batch, optimizer_disc_class, 
                    proportions_per_feature, expected_sigmatrix, epochs=128):
    """
    è®­ç»ƒé˜¶æ®µ 1
    :param model: AutoEncoder æ¨¡å‹
    :param train_loader: æ•°æ®åŠ è½½å™¨
    :param optimizer_ae: è‡ªç¼–ç å™¨ä¼˜åŒ–å™¨
    :param optimizer_disc_batch: æ‰¹æ¬¡åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨
    :param optimizer_disc_class: ç±»åˆ«åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨
    :param proportions_per_feature: æ¯ä¸ªç‰¹å¾çš„è°ƒæ•´æ¯”ä¾‹ï¼Œå½¢çŠ¶ä¸º (feature_dim, num_classes, num_genes)
    :param expected_sigmatrix: å¤–éƒ¨ä¼ å…¥çš„ sigmatrixï¼Œå½¢çŠ¶ä¸º (feature_dim, num_genes)
    :param epochs: è®­ç»ƒè½®æ•°
    :return: è®­ç»ƒåçš„æ¨¡å‹åŠå„æŸå¤±åˆ—è¡¨
    """
    model.train()
    model.state = 'train'

    # åˆå§‹åŒ–æŸå¤±å‡½æ•°
    criterion_recon = nn.MSELoss()
    criterion_disc = nn.CrossEntropyLoss()
    criterion_sigmatrix = nn.MSELoss()
    criterion_intermediate = nn.MSELoss()

    # åˆå§‹åŒ–æŸå¤±è®°å½•
    recon_loss_all = []
    adv_loss = []
    class_loss = []
    sigmatrix_loss_all = []
    intermediate_loss_all = []
  
    # åœ¨è®­ç»ƒå¾ªç¯å¤–éƒ¨åˆå§‹åŒ–ç±»ä¸­å¿ƒå’Œæ ·æœ¬è®¡æ•°å™¨
    class_centers = {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªç±»çš„ä¸­å¿ƒï¼Œé”®ä¸ºç±»åˆ«ç´¢å¼•ï¼Œå€¼ä¸ºä¸­å¿ƒå‘é‡
    class_sample_counts = {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªç±»çš„æ ·æœ¬æ•°é‡

    for epoch in tqdm(range(epochs)):
        for x, class_label, batch_label in train_loader:
            
            ### Step 1: è®­ç»ƒæ‰¹æ¬¡åˆ¤åˆ«å™¨ ###
            for param in model.encoder.parameters():
                param.requires_grad = False
            for param in model.decoder.parameters():
                param.requires_grad = False
            for param in model.batch_discriminator.parameters():
                param.requires_grad = True
            for param in model.class_discriminator.parameters():
                param.requires_grad = False

            optimizer_disc_batch.zero_grad()

            z = model.encode(x)
            batch_pred = model.batch_discriminator(z)
            batch_disc_loss = criterion_disc(batch_pred, batch_label)
            batch_disc_loss.backward()
            optimizer_disc_batch.step()

            adv_loss.append(batch_disc_loss.item())

            ### Step 2: è®­ç»ƒç±»åˆ«åˆ¤åˆ«å™¨ ###
            for param in model.encoder.parameters():
                param.requires_grad = False
            for param in model.decoder.parameters():
                param.requires_grad = False
            for param in model.batch_discriminator.parameters():
                param.requires_grad = False
            for param in model.class_discriminator.parameters():
                param.requires_grad = True

            optimizer_disc_class.zero_grad()

            z = model.encode(x)
            class_pred = model.class_discriminator(z)
            class_disc_loss = criterion_disc(class_pred, class_label)
            class_disc_loss.backward()
            optimizer_disc_class.step()

            class_loss.append(class_disc_loss.item())

            ### Step 3: è®­ç»ƒè‡ªç¼–ç å™¨ ###
            for param in model.encoder.parameters():
                param.requires_grad = True
            for param in model.decoder.parameters():
                param.requires_grad = True
            for param in model.batch_discriminator.parameters():
                param.requires_grad = False
            for param in model.class_discriminator.parameters():
                param.requires_grad = False

            optimizer_ae.zero_grad()

            # æä¾›è°ƒæ•´ç‰¹å¾çš„å‚æ•°
            class_sub = class_label

            # å‰å‘ä¼ æ’­
            #x_recon, z, batch_pred, class_pred, sigmatrix, intermediate_result = model(x, expected_sigmatrix, class_sub, proportions_per_feature)
            x_recon, z, batch_pred, class_pred, sigmatrix, intermediate_result = model(x, expected_sigmatrix, class_sub, proportions_per_feature, calculate_intermediate=True)

            # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
            #print(f"sigmatrix shape: {sigmatrix.shape}")
            #print(f"expected_sigmatrix shape: {expected_sigmatrix.shape}")
            #print(f"x_recon shape: {x_recon.shape}")   
            #print(f"x shape: {x.shape}")              
            #print(f"z shape: {z.shape}")         
            #print(f"batch_pred shape: {batch_pred.shape}")    
            #print(f"batch_label shape: {batch_label.shape}")                
            #print(f"class_pred shape: {class_pred.shape}")      
            #print(f"class_label shape: {class_label.shape}")                
            #print(f"intermediate_result shape: {intermediate_result.shape}")                     
            
            # é‡æ„æŸå¤±
            recon_loss = criterion_recon(x_recon, x)
            
            #ä¸æ¯”ä¾‹è®¡ç®—å¾—åˆ°çš„æŸå¤±
            #recon_loss2 = criterion_recon(x_recon, x_recon2)           
            #sigmatrix_loss_all.append(recon_loss2.item())

            # ç±»åˆ«åˆ¤åˆ«å™¨æŸå¤±
            class_disc_loss_ae = criterion_disc(class_pred, class_label)

            # æ‰¹æ¬¡åˆ¤åˆ«å™¨æŸå¤±
            batch_disc_loss_ae = criterion_disc(batch_pred, batch_label)
            
            # Sigmatrix æŸå¤±   
            sigmatrix_loss = criterion_sigmatrix(sigmatrix, expected_sigmatrix)
            sigmatrix_loss_all.append(sigmatrix_loss.item())

            # Intermediate result æŸå¤±
            intermediate_loss = 0
            # éå†æ¯ä¸ªæ ·æœ¬
            for i in range(intermediate_result.size(0)):  # batch_size æ¬¡å¾ªç¯

                # å½“å‰æ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾
                current_features = intermediate_result[i]  # (feature_dim, num_genes)

                # è®¡ç®—ç‰¹å¾ä¸¤ä¸¤å·®å¼‚çš„å¹³æ–¹ï¼Œä¿ç•™æ‰€æœ‰ pair-wise å·®å¼‚çš„å‡å€¼
                feature_differences = torch.cdist(current_features, current_features, p=2)  # (feature_dim, feature_dim)
                feature_mse = torch.mean(feature_differences ** 2)  # å¹³å‡å·®å¼‚

                # ç´¯åŠ å½“å‰æ ·æœ¬çš„æŸå¤±
                intermediate_loss += feature_mse

                # å¯¹ batch_size æ±‚å¹³å‡
                intermediate_loss /= intermediate_result.size(0)            
            
            
            #for i in range(intermediate_result.size(0)):
            #    feature_means = intermediate_result[i].mean(dim=1)
                # æ£€æŸ¥ intermediate_result å’Œ feature_means çš„å½¢çŠ¶æ˜¯å¦åŒ¹é…           
            #    print(f"intermediate_result shape: {intermediate_result[i].shape}")
            #    print(f"feature_means shape: {feature_means.unsqueeze(1).shape}")
            #    intermediate_loss += criterion_intermediate(intermediate_result[i], feature_means.unsqueeze(1))
            #intermediate_loss /= intermediate_result.size(0)
            intermediate_loss_all.append(intermediate_loss.item())
            
            # è®¡ç®—ç±»ä¸­å¿ƒæŸå¤±
            class_center_loss = 0.0
            for class_id in class_label.unique():
                # è·å–å½“å‰ç±»çš„æ ·æœ¬
                class_mask = (class_label == class_id)
                class_samples = z[class_mask]  # å½“å‰ç±»çš„æ ·æœ¬ç‰¹å¾ (num_samples_in_class, feature_dim)

                # å½“å‰æ‰¹æ¬¡çš„ç±»å‡å€¼
                current_class_mean = class_samples.mean(dim=0)

                if class_id.item() not in class_centers:
                    # å¦‚æœç±»ä¸­å¿ƒå°šæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å½“å‰æ‰¹æ¬¡å‡å€¼åˆå§‹åŒ–
                    class_centers[class_id.item()] = current_class_mean.detach()  # åˆ†ç¦»è®¡ç®—å›¾
                    class_sample_counts[class_id.item()] = class_samples.size(0)
                else:
                    # å¦‚æœç±»ä¸­å¿ƒå·²åˆå§‹åŒ–ï¼Œä½¿ç”¨åŠ æƒå¹³å‡æ›´æ–°
                    prev_center = class_centers[class_id.item()]
                    prev_count = class_sample_counts[class_id.item()]
                    total_count = prev_count + class_samples.size(0)

                    # æ›´æ–°ç±»ä¸­å¿ƒ
                    class_centers[class_id.item()] = (
                        (prev_center * prev_count + current_class_mean * class_samples.size(0)) / total_count
                    ).detach()  # ç¡®ä¿åˆ†ç¦»è®¡ç®—å›¾
                    class_sample_counts[class_id.item()] = total_count

                # è®¡ç®—å½“å‰ç±»æ ·æœ¬åˆ°ç±»ä¸­å¿ƒçš„è·ç¦»ï¼Œå¹¶ç´¯åŠ åˆ°æŸå¤±
                updated_center = class_centers[class_id.item()]
                class_center_loss += ((class_samples - updated_center.detach()) ** 2).sum()  # åˆ†ç¦»è®¡ç®—å›¾

            # å¹³å‡åŒ–ç±»ä¸­å¿ƒæŸå¤±
            class_center_loss /= x.size(0)
            
            # æ€»æŸå¤±
            total_loss = (
                1 * recon_loss +
                1 * class_disc_loss_ae +
                batch_disc_loss_ae +
                1 * sigmatrix_loss +
                class_center_loss +
                1 * intermediate_loss
            )

            total_loss.backward()
            optimizer_ae.step()

            recon_loss_all.append(recon_loss.item())

    return model, recon_loss_all, adv_loss, class_loss, sigmatrix_loss_all, intermediate_loss_all, class_centers

def train_model1(train_x, class_all, batch_labels, proportions_per_feature, expected_sigmatrix, 
                 model_name=None, batch_size=128, epochs=128):
    """
    è®­ç»ƒè‡ªç¼–ç å™¨æ¨¡å‹ã€‚
    :param train_x: è¾“å…¥ç‰¹å¾æ•°æ®ï¼Œå½¢çŠ¶ä¸º (æ ·æœ¬æ•°, ç‰¹å¾æ•°)
    :param class_all: æ¯ä¸ªæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾
    :param batch_labels: æ¯ä¸ªæ ·æœ¬çš„æ‰¹æ¬¡æ ‡ç­¾
    :param proportions_per_feature: æ¯ä¸ªç‰¹å¾çš„è°ƒæ•´æ¯”ä¾‹ï¼Œå½¢çŠ¶ä¸º (feature_dim, num_classes, num_genes)
    :param expected_sigmatrix: å¤–éƒ¨ä¼ å…¥çš„ sigmatrixï¼Œå½¢çŠ¶ä¸º (feature_dim, num_genes)
    :param model_name: ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶å
    :param batch_size: æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°
    :param epochs: è®­ç»ƒè½®æ•°
    :return: è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(SimDataset1(train_x, class_all, batch_labels), 
                              batch_size=batch_size, shuffle=True)
    
    # åˆå§‹åŒ–æ¨¡å‹
    num_classes = len(np.unique(class_all))
    num_batches = len(np.unique(batch_labels))
    model = AutoEncoder(train_x.shape[1], 3, num_batches,num_classes).to(device)

    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer_ae = Adam(list(model.encoder.parameters()) + list(model.decoder.parameters()), lr=1e-4)
    optimizer_disc_batch = Adam(model.batch_discriminator.parameters(), lr=1e-4)
    optimizer_disc_class = Adam(model.class_discriminator.parameters(), lr=1e-4)

    print('Start training...')


    # è°ƒç”¨è®­ç»ƒå‡½æ•°
    model, recon_loss_all, adv_loss, class_loss, sigmatrix_loss_all, intermediate_loss_all,class_centers = training_stage1(
        model, train_loader, optimizer_ae, optimizer_disc_batch, optimizer_disc_class,
        proportions_per_feature, expected_sigmatrix, epochs=epochs)
    

    # æ‰“å°æŸå¤±
    print('Reconstruction loss:')
    showloss(recon_loss_all)
    print('Adversarial loss:')
    showloss(adv_loss)
    print('Class loss:')
    showloss(class_loss)
    print('Sigmatrix loss:')
    showloss(sigmatrix_loss_all)
    print('Intermediate result loss:')
    showloss(intermediate_loss_all)
    
    # ä¿å­˜æ¨¡å‹
    if model_name is not None:
        torch.save(model.state_dict(), model_name + ".pth")
    
    return model,class_centers

def run_logfc_analysis_and_generate_fc_array(
    sample_metadata,
    expr_matrix,
    output_path
):
    """
    ä»è¡¨å‹å’Œè¡¨è¾¾çŸ©é˜µä¸­è¿è¡Œ shared category logFC åˆ†æï¼Œå¹¶ç”Ÿæˆ fold-change ä¸‰ç»´æ•°ç»„ã€‚
    
    å‚æ•°:
        sample_metadata: DataFrameï¼Œå« disease åˆ†ç±»åˆ—
        expr_matrix: DataFrameï¼Œè¡Œä¸ºåŸºå› ï¼Œåˆ—ä¸ºæ ·æœ¬
        output_path: strï¼Œè¾“å‡ºç»“æœæ–‡ä»¶å¤¹
    
    è¿”å›:
        proportions_array_fc: np.ndarray, shape = (ç–¾ç—…æ•°, ç±»åˆ«æ•°, åŸºå› æ•°)
        top3_diseases: listï¼Œç–¾ç—…åé¡ºåº
        fixed_categories: listï¼Œç±»åˆ«é¡ºåº
        fixed_genes: listï¼ŒåŸºå› é¡ºåº
    """

    # Label ç¼–ç 
    le = LabelEncoder()
    class_labels = le.fit_transform(sample_metadata['disease'])
    class_names = le.inverse_transform(class_labels)
    fixed_categories = list(le.classes_)

    # Top N ç–¾ç—…ç±»åˆ«
    top3_diseases = sample_metadata['disease'].value_counts().head(3).index.tolist()

    # åŠ è½½ R è„šæœ¬å¹¶è·å–å‡½æ•°
    # ä½¿ç”¨ importlib.resources è·å– R è„šæœ¬çš„è·¯å¾„
    with resources.path('deepadvancer', 'shared_logFC_analysis.R') as r_script_path:
        r.source(str(r_script_path))
    #robjects.r['source']('/media/lab_chen/7a17f691-f95e-41bc-93b9-865a0241ff7a/CMT/Multi-agent/test/Basic_code/shared_logFC_analysis.R')
    run_logFC = robjects.globalenv['run_shared_category_logFC_analysis']

    # è½¬æ¢è¡¨å‹å’Œè¡¨è¾¾çŸ©é˜µä¸º R å¯¹è±¡
    with localconverter(default_converter + pandas2ri.converter):
        r_phenotype = pandas2ri.py2rpy(sample_metadata)
        r_expression = pandas2ri.py2rpy(expr_matrix)

    # è¿è¡Œåˆ†æ
    for disease in top3_diseases:
        print(f"ğŸ” æ­£åœ¨åˆ†æ: {disease} ...")
        result = run_logFC(r_phenotype, r_expression, disease, output_path, 0.3)
        output_file = result.rx2('output_file')[0]
        print(f"âœ… ä¿å­˜è‡³: {output_file}")
 
    def clean_disease_name(name):
        return name.replace('.', '_').replace('-', '_')
        
    # æ„å»º {æ¸…ç†ååç§°: åŸå§‹åç§°} çš„åæŸ¥è¡¨
    reverse_name_map = {clean_disease_name(d): d for d in top3_diseases}
        
    merged_results_dict = {}
    file_list = glob.glob(os.path.join(output_path, '*_merged_results.csv'))

    for file_path in file_list:
        file_name = os.path.basename(file_path)
        cleaned_name = file_name.replace('_merged_results.csv', '')
        original_name = reverse_name_map[cleaned_name]

        try:
            df = pd.read_csv(file_path, index_col=0)
            merged_results_dict[original_name] = df
            print(f"âœ… æˆåŠŸè¯»å–: {file_name} -> æ˜ å°„ä¸º: {original_name}")
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥: {file_name}ï¼Œé”™è¯¯ä¿¡æ¯: {e}")


    # åŸºå‡†å¯¹é½
    fixed_categories = list(le.classes_) 
    fixed_genes = list(expr_matrix.index)      

    def logfc_to_fc_clipped(logfc_df, clip_val=5):
        logfc_clipped = logfc_df.clip(lower=-clip_val, upper=clip_val)
        return 2 ** logfc_clipped

    # åˆå§‹åŒ– 3D æ•°ç»„
    num_diseases = len(top3_diseases)
    num_categories = len(fixed_categories)
    num_genes = len(fixed_genes)
    proportions_array_fc = np.empty((num_diseases, num_categories, num_genes))

    for i, disease in enumerate(top3_diseases):
        df_logfc = merged_results_dict[disease]
        df_logfc_aligned = df_logfc.reindex(index=fixed_genes, columns=fixed_categories)
        df_fc = logfc_to_fc_clipped(df_logfc_aligned)
        proportions_array_fc[i] = df_fc.T.values  # ç±»åˆ« Ã— åŸºå› 

    proportions_array_fc = np.nan_to_num(proportions_array_fc, nan=1)

    print("âœ… proportions_array_fc.shape =", proportions_array_fc.shape)
    
    

    # åŠ è½½ R è„šæœ¬ï¼Œç¡®ä¿å…¶ä¸­å®šä¹‰äº† process_batch_correction1 å‡½æ•°
    #robjects.r['source']('/media/lab_chen/7a17f691-f95e-41bc-93b9-865a0241ff7a/CMT/Multi-agent/test/Basic_code/process_batch_correction.R')
    with resources.path('deepadvancer', 'process_batch_correction.R') as r_script_path:
        r.source(str(r_script_path))
    # è·å–å‡½æ•°å¼•ç”¨
    process_batch = robjects.globalenv['process_batch_correction1']


    # è½¬æ¢ä¸º R å¯¹è±¡
    with localconverter(default_converter + pandas2ri.converter):
        r_pheno = pandas2ri.py2rpy(sample_metadata )
        r_expr = pandas2ri.py2rpy(expr_matrix)

    # è®¾ç½®å‚æ•°
    target_disease = top3_diseases[0]
    shared_disease = top3_diseases[1]
    secondary_disease = top3_diseases[2]

    # è°ƒç”¨ R å‡½æ•°
    result = process_batch(target_disease, shared_disease, secondary_disease, r_pheno, r_expr)

    # æå–è¿”å›çš„ R å¯¹è±¡ä¸­çš„ 3 ä¸ªéƒ¨åˆ†
    subset_expr = result.rx2('subset_expression')
    corrected_expr = result.rx2('corrected_expression')
    subset_meta = result.rx2('subset_data')

    # è½¬å› pandas
    with localconverter(default_converter + pandas2ri.converter):
        df_expr = pandas2ri.rpy2py(corrected_expr)
        df_meta = pandas2ri.rpy2py(subset_meta)

    # ç¡®ä¿åˆ—åå¯¹é½
    df_expr = pd.DataFrame(df_expr, columns=df_meta['geo_accession'].values, index=expr_matrix.index)


    # è·å–ä¸‰ä¸ªç±»åˆ«æ ·æœ¬
    samples1 = df_meta[df_meta['disease'] == top3_diseases[0]]['geo_accession']
    samples2 = df_meta[df_meta['disease'] == top3_diseases[1]]['geo_accession']
    samples3 = df_meta[df_meta['disease'] == top3_diseases[2]]['geo_accession']

    # æå–å­çŸ©é˜µ
    expr1 = df_expr[samples1]
    expr2 = df_expr[samples2]
    expr3 = df_expr[samples3]

    # è®¡ç®—ä¸­å¿ƒè¡¨è¾¾çŸ©é˜µ
    center_exp = pd.DataFrame({
        top3_diseases[0]: expr1.mean(axis=1),
        top3_diseases[1]: expr2.mean(axis=1),
        top3_diseases[2]: expr3.mean(axis=1)
    }).T


    # åˆ›å»ºå­—å…¸å­˜å‚¨ source_dataset å¯¹åº”çš„ disease
    dataset_disease_mapping = defaultdict(set)

    # éå†æ¯ä¸€è¡Œï¼Œå¡«å……å­—å…¸
    for _, row in sample_metadata.iterrows():
        dataset_disease_mapping[row['source_dataset']].add(row['disease'])

    # æ„å»ºæ¯ä¸ª top3_diseases ç›¸å…³è”çš„ disease é›†åˆ
    related_diseases_dict = {}

    for disease in top3_diseases:
        datasets_with_disease = {
            dataset: diseases for dataset, diseases in dataset_disease_mapping.items()
            if disease in diseases
        }
    
        related = set()
        for dataset, diseases in datasets_with_disease.items():
            related.update(diseases - {disease})
    
        related_diseases_dict[disease] = related
        #print(f"Diseases that appear with '{disease}' in the same dataset:")
        #print(", ".join(sorted(related)))



    # æå–å¯¹åº”é›†åˆ
    rel_healthy = related_diseases_dict[top3_diseases[0]]
    rel_psoriasis = related_diseases_dict[top3_diseases[1]]
    rel_ad = related_diseases_dict[top3_diseases[2]]

    # äº¤é›†è¿ç®—
    three_class_shared = rel_healthy & rel_psoriasis & rel_ad
    three_class_shared.update(top3_diseases)


    feature1_feature2_shared = rel_healthy & rel_psoriasis
    feature1_feature3_shared = rel_healthy & rel_ad
    feature2_feature3_shared = rel_psoriasis & rel_ad




    # è°ƒæ•´è¡Œé¡ºåºå’Œåˆ—é¡ºåº
    df_exp_aligned = center_exp.reindex(index=top3_diseases, columns=fixed_genes)


    # è·å–æ•´ä¸ª DataFrame çš„å…¨å±€æœ€å°å€¼ä¸æœ€å¤§å€¼
    global_min = df_exp_aligned.values.min()
    global_max = df_exp_aligned.values.max()

    # æ ‡å‡†åŒ–è®¡ç®—ï¼šæ¯ä¸ªå…ƒç´ å‡å»æœ€å°å€¼å†é™¤ä»¥èŒƒå›´
    sigmatrix = (df_exp_aligned - global_min) / (global_max - global_min)
    sigmatrix


    # åˆå§‹åŒ–
    max_iterations = 100
    initial_learning_rate = 0.001
    final_learning_rate = 0.001
    initial_regularization_weight = 0.0001
    final_regularization_weight = 0.001

    # å°† DataFrame è½¬ä¸º numpyï¼Œé˜²æ­¢ç´¢å¼•é”™è¯¯
    sigmatrix_new = sigmatrix.values.copy()
    sigmatrix_orig = sigmatrix.values.copy()  # ç”¨äºæ­£åˆ™é¡¹å’Œèåˆ

    # è·å–ç±»åˆ«ç´¢å¼•
    def get_class_indices(shared_set):
        return [
            np.where(le.classes_ == cls)[0][0]
            for cls in shared_set if cls in le.classes_
        ]

    three_class_shared_indices = get_class_indices(three_class_shared)
    feature1_feature2_shared_indices = get_class_indices(feature1_feature2_shared)
    feature1_feature3_shared_indices = get_class_indices(feature1_feature3_shared)
    feature2_feature3_shared_indices = get_class_indices(feature2_feature3_shared)

    # ä¼˜åŒ– sigmatrix
    for iteration in range(max_iterations):
        # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–æƒé‡
        learning_rate = initial_learning_rate - (initial_learning_rate - final_learning_rate) * (iteration / max_iterations)
        regularization_weight = initial_regularization_weight + (
            (final_regularization_weight - initial_regularization_weight) * iteration / max_iterations
        )

        total_loss = 0
        correlation_loss = 0

        # ä¸‰ä¸ªç‰¹å¾åŒæ—¶å…±ç°çš„ç±»åˆ«
        for target_cls_index in three_class_shared_indices:
            values = np.array([
                sigmatrix_new[cls_idx, :] * proportions_array_fc[cls_idx, target_cls_index, :]
                for cls_idx in range(sigmatrix_new.shape[0])
            ])  # shape: (3, 12754)

            # Pearson ç›¸å…³æ€§è®¡ç®—ï¼ˆåŒ…å« NaN é˜²æŠ¤ï¼‰
            corr_01 = np.nan_to_num(pearsonr(values[0], values[1])[0])
            corr_02 = np.nan_to_num(pearsonr(values[0], values[2])[0])
            corr_12 = np.nan_to_num(pearsonr(values[1], values[2])[0])

            loss_corr = -(corr_01 + corr_02 + corr_12)
            correlation_loss += loss_corr

            # æ¢¯åº¦ä¼°ç®—ï¼ˆä¸­å¿ƒåŒ–å·®å¼‚ï¼‰
            mean_values = values.mean(axis=0)
            grad = np.stack([v - mean_values for v in values])
            grad += regularization_weight * (sigmatrix_new - sigmatrix_orig)

            # éšæœºæ‰°åŠ¨
            noise = np.random.normal(0, 0.0001, size=sigmatrix_new.shape)
            sigmatrix_new -= learning_rate * grad + noise

        # ç‰¹å¾ 1 å’Œ 2
        for target_cls_index in feature1_feature2_shared_indices:
            values = np.array([
                sigmatrix_new[0, :] * proportions_array_fc[0, target_cls_index, :],
                sigmatrix_new[1, :] * proportions_array_fc[1, target_cls_index, :]
            ])
            corr = np.nan_to_num(pearsonr(values[0], values[1])[0])
            loss_corr = -corr
            correlation_loss += loss_corr

            mean_values = values.mean(axis=0)
            grad = np.stack([v - mean_values for v in values])
            grad += regularization_weight * (sigmatrix_new[[0, 1], :] - sigmatrix_orig[[0, 1], :])
            sigmatrix_new[[0, 1], :] -= learning_rate * grad

        # ç‰¹å¾ 1 å’Œ 3
        for target_cls_index in feature1_feature3_shared_indices:
            values = np.array([
                sigmatrix_new[0, :] * proportions_array_fc[0, target_cls_index, :],
                sigmatrix_new[2, :] * proportions_array_fc[2, target_cls_index, :]
            ])
            corr = np.nan_to_num(pearsonr(values[0], values[1])[0])
            loss_corr = -corr
            correlation_loss += loss_corr

            mean_values = values.mean(axis=0)
            grad = np.stack([v - mean_values for v in values])
            grad += regularization_weight * (sigmatrix_new[[0, 2], :] - sigmatrix_orig[[0, 2], :])
            sigmatrix_new[[0, 2], :] -= learning_rate * grad

        # ç‰¹å¾ 2 å’Œ 3
        for target_cls_index in feature2_feature3_shared_indices:
            values = np.array([
                sigmatrix_new[1, :] * proportions_array_fc[1, target_cls_index, :],
                sigmatrix_new[2, :] * proportions_array_fc[2, target_cls_index, :]
            ])
            corr = np.nan_to_num(pearsonr(values[0], values[1])[0])
            loss_corr = -corr
            correlation_loss += loss_corr

            mean_values = values.mean(axis=0)
            grad = np.stack([v - mean_values for v in values])
            grad += regularization_weight * (sigmatrix_new[[1, 2], :] - sigmatrix_orig[[1, 2], :])
            sigmatrix_new[[1, 2], :] -= learning_rate * grad

        # --- æ¯è½®è¿­ä»£åç»Ÿä¸€åšæ­£åˆ™é¡¹ + å¹³æ»‘èåˆ ---
        regularization_loss = regularization_weight * np.linalg.norm(sigmatrix_new - sigmatrix_orig)
        total_loss = correlation_loss + regularization_loss

        # éè´Ÿè£å‰ª
        sigmatrix_new = np.clip(sigmatrix_new, 0, None)

        # èåˆåŸå§‹å€¼ï¼ˆå¹³æ»‘ï¼‰
        sigmatrix_new = 0.9 * sigmatrix_new + 0.1 * sigmatrix_orig
        print(f"[Iter {iteration+1}] Corr Loss: {correlation_loss:.4f}, Reg Loss: {regularization_loss:.4f}, Total: {total_loss:.4f}")

    
    
    # ä¿å­˜ç»“æœ
    sigmatrix_df = pd.DataFrame(sigmatrix_new, index=top3_diseases, columns=fixed_genes)
    sigmatrix_df.to_csv(os.path.join(output_path, 'sigmatrix.csv'))
    print("âœ… sigmatrixè®¡ç®—å®Œæˆ")
    
    
    
    # æŒ‰æ•°æ®é›†è¿›è¡Œ min-max å½’ä¸€åŒ–
    normalized_datasets = []    
    for dataset_name, group in sample_metadata.groupby('source_dataset'):
        sample_ids = group.index.tolist()
        valid_samples = [s for s in sample_ids if s in expr_matrix.columns]
        if not valid_samples:
            continue
        sub_expr = expr_matrix[valid_samples].copy()
        dataset_max = sub_expr.values.max()
        dataset_min = sub_expr.values.min()
        if dataset_max == dataset_min:
            print(f"âš ï¸ ä¸¢å¼ƒæ— ä¿¡æ¯æ•°æ®é›†: {dataset_name}")
            continue
        sub_expr_scaled = (sub_expr - dataset_min) / (dataset_max - dataset_min)
        normalized_datasets.append(sub_expr_scaled)

    final_expression_matrix_scaled = pd.concat(normalized_datasets, axis=1)
    final_expression_matrix_scaled

    train_x = final_expression_matrix_scaled.values.T.astype(np.float32)


    class_all = le.fit_transform(sample_metadata["disease"])
    batch_le = LabelEncoder()
    batch_labels = batch_le.fit_transform(sample_metadata["source_dataset"])
    proportions_per_feature = torch.tensor(proportions_array_fc, dtype=torch.float32).to(device)
    expected_sigmatrix = torch.tensor(sigmatrix_new, dtype=torch.float32).to(device)



    return  train_x, class_all, batch_labels, proportions_per_feature, expected_sigmatrix

def recon_training(
    train_x=train_x,
    class_all=class_all,
    batch_labels=batch_labels,
    proportions_per_feature=proportions_per_feature,
    expected_sigmatrix=expected_sigmatrix,
    output_path,
    batch_size=128,
    epochs=300
):   
    model, class_centers = train_model1(
        train_x=train_x,
        class_all=class_all,
        batch_labels=batch_labels,
        proportions_per_feature=proportions_per_feature,
        expected_sigmatrix=expected_sigmatrix,
        model_name=os.path.join(output_path, 'mymodel'),  # å¯é€‰
        batch_size=batch_size,
        epochs=epochs)

    model.eval()
    with torch.no_grad():
        x_tensor = torch.from_numpy(train_x).float().to(device)
        class_tensor = torch.tensor(class_all).long().to(device)

       # æ¨ç†è°ƒç”¨
        x_recon, _, _, _, _, _ = model(
            x_tensor,
            expected_sigmatrix=expected_sigmatrix,
            class_sub=class_tensor,
            proportions_per_feature=proportions_per_feature,
            calculate_intermediate=True # è®¾ç½®ä¸º False è¡¨ç¤ºä¸è¿”å›ä¸­é—´ç»“æœ
        )

    # è½¬ä¸º numpy ä»¥ä¾¿ä¿å­˜/åˆ†æ
    x_recon_np = x_recon.cpu().numpy()
    # è½¬ç½® reconstructed çŸ©é˜µï¼Œä½¿å…¶ä¸ expr_matrix_filtered çš„ç»“æ„ä¸€è‡´ï¼šåŸºå› åœ¨è¡Œï¼Œæ ·æœ¬åœ¨åˆ—
    x_recon_expression_matrix = pd.DataFrame(
        x_recon_np.T,  # è½¬ç½®ï¼Œä½¿å½¢çŠ¶å˜æˆ (åŸºå› æ•°, æ ·æœ¬æ•°)
        index=expr_matrix.index,  # åŸºå› å
        columns=expr_matrix.columns  # æ ·æœ¬å
    )    
    return  x_recon_expression_matrix, model   

def compute_logfc_between_classes(
    expression_matrix: pd.DataFrame,
    phenotype_metadata: pd.DataFrame,
    class_column: str,
    class1: str,
    class2: str
) -> pd.DataFrame:
    """
    è®¡ç®—ä¸¤ä¸ªç±»ä¹‹é—´çš„ log2 fold changeï¼ˆlogFCï¼‰ã€‚
    
    å‚æ•°ï¼š
        expression_matrix: DataFrameï¼Œè¡Œä¸ºåŸºå› ï¼Œåˆ—ä¸ºæ ·æœ¬
        phenotype_metadata: DataFrameï¼Œç´¢å¼•ä¸ºæ ·æœ¬åï¼ŒåŒ…å«ç±»åˆ«ä¿¡æ¯
        class_column: strï¼Œç±»åˆ«æ‰€åœ¨çš„åˆ—å
        class1: strï¼Œç¬¬ä¸€ä¸ªç±»åï¼ˆä½œä¸ºåˆ†æ¯ï¼‰
        class2: strï¼Œç¬¬äºŒä¸ªç±»åï¼ˆä½œä¸ºåˆ†å­ï¼‰
        
    è¿”å›ï¼š
        DataFrameï¼ŒåŒ…å« log2FC åˆ—ï¼Œç´¢å¼•ä¸ºåŸºå› å
    """

    # è·å–ä¸¤ä¸ªç±»çš„æ ·æœ¬ç´¢å¼•
    samples1 = phenotype_metadata[phenotype_metadata[class_column] == class1].index
    samples2 = phenotype_metadata[phenotype_metadata[class_column] == class2].index

    # æå–è¡¨è¾¾å­çŸ©é˜µ
    data1 = expression_matrix[samples1]
    data2 = expression_matrix[samples2]

    # è®¡ç®—å¹³å‡è¡¨è¾¾
    mean1 = data1.mean(axis=1)
    mean2 = data2.mean(axis=1)

    # è®¡ç®— log2 fold change
    logfc = np.log2(mean2 + 1e-8) - np.log2(mean1 + 1e-8)

    # å°è£…ä¸º DataFrame
    result = pd.DataFrame({f"log2FC_{class2}_vs_{class1}": logfc})
    return result.sort_values(by=result.columns[0], ascending=False)

def compute_logfc_vs_others(
    expression_matrix: pd.DataFrame,
    phenotype_metadata: pd.DataFrame,
    class_column: str,
    target_class: str
) -> pd.DataFrame:
    """
    è®¡ç®—ç›®æ ‡ç±» vs æ‰€æœ‰å…¶ä»–ç±»çš„ log2 fold changeã€‚
    
    å‚æ•°ï¼š
        expression_matrix: DataFrameï¼Œè¡Œä¸ºåŸºå› ï¼Œåˆ—ä¸ºæ ·æœ¬
        phenotype_metadata: DataFrameï¼Œç´¢å¼•ä¸ºæ ·æœ¬åï¼ŒåŒ…å«ç±»åˆ«ä¿¡æ¯
        class_column: strï¼Œç±»åˆ«æ‰€åœ¨çš„åˆ—å
        target_class: strï¼Œè¦æ¯”è¾ƒçš„ç±»å
        
    è¿”å›ï¼š
        DataFrameï¼Œindex ä¸ºåŸºå› ï¼Œcolumns ä¸ºå…¶ä»–ç±»åˆ«åï¼Œæ¯åˆ—æ˜¯ target_class vs other_class çš„ log2FC
    """
    # æå–ç›®æ ‡ç±»æ ·æœ¬å¹¶è®¡ç®—å¹³å‡è¡¨è¾¾
    target_samples = phenotype_metadata[phenotype_metadata[class_column] == target_class].index
    target_data = expression_matrix[target_samples]
    mean_target = target_data.mean(axis=1)

    # åˆå§‹åŒ–ç»“æœ DataFrame
    logfc_df = pd.DataFrame(index=expression_matrix.index)

    # éå†æ‰€æœ‰å…¶ä»–ç±»
    for other_class in phenotype_metadata[class_column].unique():
        if other_class == target_class:
            continue
        other_samples = phenotype_metadata[phenotype_metadata[class_column] == other_class].index
        other_data = expression_matrix[other_samples]
        mean_other = other_data.mean(axis=1)

        # è®¡ç®— logFC
        logfc = np.log2(mean_target + 1e-8) - np.log2(mean_other + 1e-8)
        logfc_df[f'{target_class}_vs_{other_class}'] = logfc

    return logfc_df

